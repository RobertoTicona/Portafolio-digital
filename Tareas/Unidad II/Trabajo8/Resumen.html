<!DOCTYPE html>
<html lang="es">
<head>
  <meta charset="UTF-8">
  <title>Comparaci√≥n de M√©todos de Regresi√≥n Lineal ‚Äì Programaci√≥n Num√©rica</title>
  <link rel="stylesheet" href="../../../css/tareas.css">
</head>
<body>

  <!-- Encabezado -->
  <header>
    <h1>Unidad II ‚Äì An√°lisis del Art√≠culo</h1>
    <p>Actividad del curso de Programaci√≥n Num√©rica</p>
  </header>

  <!-- Descripci√≥n -->
  <div class="section">
    <h2>Descripci√≥n</h2>
    <p style="text-align: justify;">
      El art√≠culo compara dos m√©todos fundamentales para resolver problemas de regresi√≥n lineal: el 
      <strong>pseudoinverso de Moore‚ÄìPenrose</strong> y el <strong>descenso de gradiente</strong>.
      La regresi√≥n lineal busca encontrar los par√°metros que mejor predicen una variable dependiente
      a partir de variables independientes, minimizando los errores cuadrados (OLS).
    </p>

    <p style="text-align: justify;">
      El pseudoinverso de Moore‚ÄìPenrose ofrece una <strong>soluci√≥n anal√≠tica exacta</strong> mediante
      operaciones matriciales, proporcionando el m√≠nimo error posible. Sin embargo, puede ser
      computacionalmente costoso e inestable cuando la matriz de caracter√≠sticas es muy grande o est√°
      mal condicionada. En cambio, el <strong>descenso de gradiente</strong> es un m√©todo 
      <strong>iterativo</strong> que ajusta los par√°metros gradualmente en funci√≥n de la direcci√≥n del
      gradiente negativo del error, controlado por una tasa de aprendizaje. Aunque no garantiza una
      soluci√≥n exacta, es escalable y adaptable a grandes vol√∫menes de datos.
    </p>

    <p style="text-align: justify;">
      En el <strong>marco te√≥rico</strong>, se explica que la estabilidad num√©rica y la eficiencia de cada
      m√©todo dependen del tama√±o del conjunto de datos (<em>n</em>), el n√∫mero de variables (<em>d</em>) y la
      condici√≥n de la matriz <em>X</em>. El pseudoinverso utiliza descomposici√≥n SVD, mientras que el descenso
      de gradiente requiere pasos controlados y puede verse afectado por la escala de las variables.
    </p>

    <p style="text-align: justify;">
      En la <strong>metodolog√≠a</strong>, el autor realiz√≥ experimentos con datos sint√©ticos y reales.
      Los datos sint√©ticos permitieron manipular par√°metros como el n√∫mero de muestras, la dimensionalidad
      y el factor de condici√≥n. Para los datos reales se usaron los conjuntos <em>California Housing</em>
      (20,640 muestras y 8 variables) y <em>Diabetes</em> (442 muestras y 10 variables). En ambos casos se
      evaluaron tres m√©tricas: <strong>tiempo de ejecuci√≥n</strong>, <strong>error cuadr√°tico medio (MSE)</strong>
      y, para el descenso de gradiente, el <strong>n√∫mero de iteraciones hasta la convergencia</strong>. 
      Los experimentos se implementaron en Python, usando la funci√≥n <code>pinv()</code> para el pseudoinverso 
      y un algoritmo de gradiente con tasa de aprendizaje <em>Œ± = 0.01</em> y tolerancia <em>10‚Åª‚Å∂</em>.
    </p>

    <p style="text-align: justify;">
      Los <strong>resultados</strong> mostraron que el pseudoinverso fue m√°s r√°pido y preciso en conjuntos
      peque√±os o moderados, manteniendo una alta estabilidad num√©rica. En cambio, el descenso de gradiente
      necesit√≥ m√°s iteraciones y su rendimiento dependi√≥ fuertemente del escalado de las variables y del
      valor de la tasa de aprendizaje. Sin embargo, en escenarios de alta dimensionalidad o grandes vol√∫menes
      de datos, el descenso de gradiente (y sus variantes como el estoc√°stico) result√≥ m√°s escalable y eficiente.
    </p>

    <p style="text-align: justify;">
      En <strong>conclusi√≥n</strong>, el pseudoinverso de Moore‚ÄìPenrose es ideal para conjuntos de datos bien
      condicionados y de tama√±o moderado, ofreciendo soluciones exactas con bajo error. El descenso de gradiente
      es preferible para problemas de gran escala, aunque requiere ajuste de hiperpar√°metros y un buen
      preprocesamiento. Ambos m√©todos son esenciales en el an√°lisis de regresi√≥n, y su elecci√≥n depende del
      equilibrio entre exactitud, estabilidad y costo computacional.
    </p>
  </div>

  <!-- Comparaci√≥n -->
  <div class="section">
    <h2>Comparaci√≥n de rendimiento entre m√©todos de optimizaci√≥n</h2>
    <p style="text-align: justify;">
      El objetivo de esta prueba fue evaluar la eficiencia computacional del m√©todo de regresi√≥n lineal de 
      <code>R</code> (<code>lm</code>) en comparaci√≥n con el algoritmo de <strong>gradiente descendente</strong>.
      Se emplearon conjuntos de datos de diferentes tama√±os (100 a 10,000 observaciones) y se registraron el 
      <strong>tiempo de ejecuci√≥n</strong> (en segundos) y el <strong>uso de memoria</strong> (en bytes) mediante 
      la librer√≠a <code>bench</code>. Ambos m√©todos fueron implementados bajo las mismas condiciones para 
      garantizar una comparaci√≥n justa.
    </p>

    <table border="1" style="border-collapse: collapse; margin: auto; text-align: center; width: 90%;">
      <thead>
        <tr>
          <th>Tama√±o</th>
          <th>M√©todo</th>
          <th>Tiempo (s)</th>
          <th>Memoria (bytes)</th>
        </tr>
      </thead>
      <tbody>
        <tr><td>100</td><td>lm</td><td>5.431</td><td>104,573,176</td></tr>
        <tr><td>100</td><td>Gradiente</td><td>0.005</td><td>143,432</td></tr>
        <tr><td>1000</td><td>lm</td><td>2.769</td><td>967,960,000</td></tr>
        <tr><td>1000</td><td>Gradiente</td><td>0.030</td><td>800,048</td></tr>
        <tr><td>5000</td><td>lm</td><td>16.215</td><td>4,934,680,000</td></tr>
        <tr><td>5000</td><td>Gradiente</td><td>0.566</td><td>4,000,048</td></tr>
        <tr><td>10000</td><td>lm</td><td>10.175</td><td>9,862,360,000</td></tr>
        <tr><td>10000</td><td>Gradiente</td><td>0.352</td><td>8,000,048</td></tr>
      </tbody>
    </table>

    <p style="text-align: justify; margin-top: 15px;">
      Los resultados evidencian una ventaja clara del gradiente descendente frente a la funci√≥n 
      <code>lm()</code>. Mientras que <code>lm()</code> presenta un aumento dr√°stico del tiempo y la memoria 
      conforme crece el tama√±o de los datos, el gradiente mantiene un incremento casi lineal y mucho menor. 
      Esto se debe a que <code>lm()</code> realiza operaciones matriciales completas para estimar los coeficientes, 
      lo que resulta costoso en t√©rminos computacionales, mientras que el gradiente ajusta los par√°metros de forma 
      iterativa y progresiva.
    </p>

    <p style="text-align: justify;">
      En consecuencia, para grandes vol√∫menes de datos o tareas repetitivas, el m√©todo del gradiente descendente 
      ofrece un rendimiento considerablemente superior, tanto en velocidad como en eficiencia del uso de recursos.
    </p>

    <div class="imagenes" style="text-align: center; margin-top: 20px;">
      <img src="tiempo.png" alt="Gr√°fico de tiempo de ejecuci√≥n" style="width:45%; margin-right:2%;">
      <img src="memoria.png" alt="Gr√°fico de uso de memoria" style="width:45%;">
    </div>
  </div>

  <div class="section">
    <div class="links">
      <a href="PN_U2_Trabajo_8_An√°lisis_del_art√≠culo.pdf" target="_blank">üìÑ Ver PDF</a>
      <a href="../../../unidad2.html">‚¨ÖÔ∏è Volver a Actividades de la Unidad 2</a>
    </div>
  </div>

</body>
</html>

